{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem 1 - Learning Rate, Batch Size, FashionMNIST\n",
    "\n",
    "Recall cyclical learning rate policy discussed in Lecture 4. The learning rate changes in cyclical manner between lrmin and lrmax, which are hyperparameters that need to be specified. For this problem you first need to read carefully the article referenced below as you will be making use of the code there (in Keras) and modifying it as needed. For those who want to work in Pytorch there are open source implementations of this policy available which you can easily search for and build over them. You will work with FashionMNIST dataset and LeNet-5.\n",
    "\n",
    "References:\n",
    "1. Leslie N. Smith Cyclical Learning Rates for Training Neural Networks. Available at https://arxiv.org/abs/1506.01186.\n",
    "2. Keras implementation of cyclical learning rate policy. Available at https://www.pyimagesearch.com/2019/08/05/keras-\n",
    "learning-rate-finder/."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "1. Fix batch size to 64 and start with 10 candidate learning rates between 10âˆ’9 and 101 and train your model for 5 epochs for each learning rate. Plot the training loss as a function of learning rate. You should see a curve like Figure 3 in reference below. From that figure identify the values of lrmin and lrmax. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "EnvironmentLocationNotFound: Not a conda environment: /Users/aragaom/opt/anaconda3/envs/Python3\n",
      "\n",
      "Installed kernelspec python3 in /usr/local/share/jupyter/kernels/python3\n",
      "\u001b[31mERROR: Could not find a version that satisfies the requirement cv2 (from versions: none)\u001b[0m\u001b[31m\n",
      "\u001b[0m\u001b[31mERROR: No matching distribution found for cv2\u001b[0m\u001b[31m\n",
      "\u001b[0m\u001b[33mWARNING: You are using pip version 22.0.3; however, version 22.3 is available.\n",
      "You should consider upgrading via the '/Users/aragaom/opt/anaconda3/bin/python -m pip install --upgrade pip' command.\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "! conda install ipykernel --name Python3\n",
    "! python -m ipykernel install\n",
    "! pip3 install cv2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import the necessary packages\n",
    "import os\n",
    "# initialize the list of class label names\n",
    "CLASSES = [\"top\", \"trouser\", \"pullover\", \"dress\", \"coat\",\n",
    "\t\"sandal\", \"shirt\", \"sneaker\", \"bag\", \"ankle boot\"]\n",
    "# define the minimum learning rate, maximum learning rate, batch size,\n",
    "# step size, CLR method, and number of epochs\n",
    "MIN_LR = 1e-5\n",
    "MAX_LR = 1e-2\n",
    "BATCH_SIZE = 64\n",
    "STEP_SIZE = 8\n",
    "CLR_METHOD = \"triangular\"\n",
    "NUM_EPOCHS = 50\n",
    "# define the path to the output learning rate finder plot, training\n",
    "# history plot and cyclical learning rate plot\n",
    "LRFIND_PLOT_PATH = os.path.sep.join([\"output\", \"lrfind_plot.png\"])\n",
    "TRAINING_PLOT_PATH = os.path.sep.join([\"output\", \"training_plot.png\"])\n",
    "CLR_PLOT_PATH = os.path.sep.join([\"output\", \"clr_plot.png\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras import datasets, layers, models, losses\n",
    "\n",
    "def create_model():\n",
    "  model = models.Sequential()\n",
    "\n",
    "  model.add(layers.Conv2D(6, 5, activation='tanh', input_shape=trainX.shape[1:]))\n",
    "  model.add(layers.AveragePooling2D(2))\n",
    "\n",
    "\n",
    "  model.add(layers.Conv2D(16, 5, activation='tanh'))\n",
    "  model.add(layers.AveragePooling2D(2))\n",
    "\n",
    "  model.add(layers.Conv2D(120, 5, activation='tanh'))\n",
    "\n",
    "  model.add(layers.Flatten()) # dense layer is a linear layer and we flatten the input before putting it there.\n",
    "  model.add(layers.Dense(84, activation='tanh'))\n",
    "\n",
    "  model.add(layers.Dense(10, activation='softmax'))\n",
    "\n",
    "  return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "def load_data_mnist_tf(batch_size, resize=None):   \n",
    "    \n",
    "    # load dataset\n",
    "    mnist_train, mnist_test = tf.keras.datasets.mnist.load_data()\n",
    "\n",
    "    # normalisation and cast as Int datatype\n",
    "    process = lambda X, y: (tf.expand_dims(X, axis=3) / 255,tf.cast(y, dtype='int32')) \n",
    "    # the pixel values must be personalized, so each feature has the same affect ont he output \n",
    "\n",
    "    # resize images if resize is not None\n",
    "    resize_fn = lambda X, y: (tf.image.resize_with_pad(X, resize, resize) if resize else X, y)\n",
    "    # resizing of the image fucntion ?? \n",
    "\n",
    "\n",
    "    # load train and test batches\n",
    "    train_iter = tf.data.Dataset.from_tensor_slices(process(*mnist_train)).batch(batch_size).shuffle(len(mnist_train[0])).map(resize_fn)\n",
    "    test_iter = tf.data.Dataset.from_tensor_slices(process(*mnist_test)).batch(batch_size).map(resize_fn)\n",
    "    \n",
    "    return (train_iter, test_iter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] loading Fashion MNIST data...\n",
      "[INFO] compiling model...\n"
     ]
    }
   ],
   "source": [
    "# set the matplotlib backend so figures can be saved in the background\n",
    "import matplotlib\n",
    "matplotlib.use(\"Agg\")\n",
    "# import the necessary packages\n",
    "from learningratefinder import LearningRateFinder\n",
    "from clr_callback import CyclicLR\n",
    "from sklearn.preprocessing import LabelBinarizer\n",
    "from sklearn.metrics import classification_report\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.optimizers import SGD\n",
    "from tensorflow.keras.datasets import fashion_mnist\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import argparse\n",
    "# import cv2\n",
    "import sys\n",
    "\n",
    "# construct the argument parser and parse the arguments\n",
    "ap = argparse.ArgumentParser()\n",
    "ap.add_argument(\"-f\", \"--lr-find\", type=int, default=0,\n",
    "\thelp=\"whether or not to find optimal learning rate\")\n",
    "args, unknown = ap.parse_known_args()\n",
    "resize_fn = lambda X, y: (tf.image.resize_with_pad(X, resize, resize) if resize else X, y)\n",
    "# load the training and testing data\n",
    "print(\"[INFO] loading Fashion MNIST data...\")\n",
    "((trainX, trainY), (testX, testY)) = fashion_mnist.load_data()\n",
    "# Fashion MNIST images are 28x28 but the network we will be training\n",
    "# is expecting 32x32 images\n",
    "# trainX = np.array([tf.image.resize(x, [32,32]) for x in trainX])\n",
    "# testX = np.array([tf.image.resize(x [32,32]) for x in testX])\n",
    "# scale the pixel intensities to the range [0, 1]\n",
    "trainX = tf.pad(trainX, [[0, 0], [2,2], [2,2]])/255\n",
    "testX = tf.pad(testX, [[0, 0], [2,2], [2,2]])/255\n",
    "\n",
    "trainX = tf.expand_dims(trainX, axis=3, name=None)\n",
    "testX = tf.expand_dims(testX, axis=3, name=None)\n",
    "\n",
    "\n",
    "# reshape the data matrices to include a channel dimension (required\n",
    "# for training)\n",
    "# trainX = trainX.reshape((trainX.shape[0], 28, 28, 1))\n",
    "# testX = testX.reshape((testX.shape[0], 28, 28, 1))\n",
    "# convert the labels from integers to vectors\n",
    "lb = LabelBinarizer()\n",
    "trainY = lb.fit_transform(trainY)\n",
    "testY = lb.transform(testY)\n",
    "# construct the image generator for data augmentation\n",
    "aug = ImageDataGenerator(width_shift_range=0.1,\n",
    "\theight_shift_range=0.1, horizontal_flip=True,\n",
    "\tfill_mode=\"nearest\")\n",
    "\n",
    "# initialize the optimizer and model\n",
    "print(\"[INFO] compiling model...\")\n",
    "opt = SGD(learning_rate=MIN_LR, momentum=0.9)\n",
    "model = create_model()\n",
    "model.compile(loss=\"categorical_crossentropy\", optimizer=opt,\n",
    "\tmetrics=[\"accuracy\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Namespace(lr_find=0)\n"
     ]
    }
   ],
   "source": [
    "print(args)\n",
    "if args.lr_find > 0:\n",
    "    \t# initialize the learning rate finder and then train with learning\n",
    "\t# rates ranging from 1e-10 to 1e+1\n",
    "\tprint(\"[INFO] finding learning rate...\")\n",
    "\tlrf = LearningRateFinder(model)\n",
    "\tlrf.find(\n",
    "\t\taug.flow(trainX, trainY, batch_size=BATCH_SIZE),\n",
    "\t\t1e-10, 1e+1,\n",
    "\t\tstepsPerEpoch=np.ceil((len(trainX) / float(BATCH_SIZE))),\n",
    "\t\tbatchSize=BATCH_SIZE)\n",
    "\t# plot the loss for the various learning rates and save the\n",
    "\t# resulting plot to disk\n",
    "\tlrf.plot_loss()\n",
    "\tplt.savefig(LRFIND_PLOT_PATH)\n",
    "\t# gracefully exit the script so we can adjust our learning rates\n",
    "\t# in the config and then train the network for our full set of\n",
    "\t# epochs\n",
    "\tprint(\"[INFO] learning rate finder complete\")\n",
    "\tprint(\"[INFO] examine plot and adjust learning rates before training\")\n",
    "\tsys.exit(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] training network...\n",
      "Epoch 1/50\n",
      "937/937 [==============================] - 21s 22ms/step - loss: 1.7772 - accuracy: 0.3730 - val_loss: 1.0360 - val_accuracy: 0.6439\n",
      "Epoch 2/50\n",
      "937/937 [==============================] - 20s 22ms/step - loss: 0.9446 - accuracy: 0.6620 - val_loss: 0.7551 - val_accuracy: 0.7149\n",
      "Epoch 3/50\n",
      "937/937 [==============================] - 21s 22ms/step - loss: 0.7540 - accuracy: 0.7158 - val_loss: 0.6526 - val_accuracy: 0.7455\n",
      "Epoch 4/50\n",
      "937/937 [==============================] - 21s 22ms/step - loss: 0.6631 - accuracy: 0.7467 - val_loss: 0.5605 - val_accuracy: 0.7920\n",
      "Epoch 5/50\n",
      "937/937 [==============================] - 21s 22ms/step - loss: 0.5935 - accuracy: 0.7734 - val_loss: 0.5279 - val_accuracy: 0.8006\n",
      "Epoch 6/50\n",
      "937/937 [==============================] - 21s 23ms/step - loss: 0.5371 - accuracy: 0.7970 - val_loss: 0.4792 - val_accuracy: 0.8195\n",
      "Epoch 7/50\n",
      "937/937 [==============================] - 20s 22ms/step - loss: 0.4968 - accuracy: 0.8117 - val_loss: 0.4641 - val_accuracy: 0.8252\n",
      "Epoch 8/50\n",
      "937/937 [==============================] - 21s 22ms/step - loss: 0.4679 - accuracy: 0.8228 - val_loss: 0.4261 - val_accuracy: 0.8421\n",
      "Epoch 9/50\n",
      "937/937 [==============================] - 22s 24ms/step - loss: 0.4410 - accuracy: 0.8335 - val_loss: 0.3928 - val_accuracy: 0.8512\n",
      "Epoch 10/50\n",
      "937/937 [==============================] - 23s 24ms/step - loss: 0.4167 - accuracy: 0.8439 - val_loss: 0.3801 - val_accuracy: 0.8556\n",
      "Epoch 11/50\n",
      "937/937 [==============================] - 22s 23ms/step - loss: 0.3932 - accuracy: 0.8527 - val_loss: 0.3753 - val_accuracy: 0.8550\n",
      "Epoch 12/50\n",
      "937/937 [==============================] - 21s 23ms/step - loss: 0.3791 - accuracy: 0.8576 - val_loss: 0.3583 - val_accuracy: 0.8667\n",
      "Epoch 13/50\n",
      "937/937 [==============================] - 21s 22ms/step - loss: 0.3658 - accuracy: 0.8635 - val_loss: 0.3530 - val_accuracy: 0.8682\n",
      "Epoch 14/50\n",
      "937/937 [==============================] - 21s 23ms/step - loss: 0.3526 - accuracy: 0.8682 - val_loss: 0.3400 - val_accuracy: 0.8742\n",
      "Epoch 15/50\n",
      "937/937 [==============================] - 21s 22ms/step - loss: 0.3433 - accuracy: 0.8717 - val_loss: 0.3357 - val_accuracy: 0.8740\n",
      "Epoch 16/50\n",
      "937/937 [==============================] - 21s 23ms/step - loss: 0.3366 - accuracy: 0.8745 - val_loss: 0.3307 - val_accuracy: 0.8748\n",
      "Epoch 17/50\n",
      "937/937 [==============================] - 22s 23ms/step - loss: 0.3317 - accuracy: 0.8759 - val_loss: 0.3355 - val_accuracy: 0.8738\n",
      "Epoch 18/50\n",
      "937/937 [==============================] - 22s 24ms/step - loss: 0.3388 - accuracy: 0.8735 - val_loss: 0.3348 - val_accuracy: 0.8742\n",
      "Epoch 19/50\n",
      "937/937 [==============================] - 21s 22ms/step - loss: 0.3432 - accuracy: 0.8720 - val_loss: 0.3458 - val_accuracy: 0.8673\n",
      "Epoch 20/50\n",
      "937/937 [==============================] - 22s 23ms/step - loss: 0.3494 - accuracy: 0.8685 - val_loss: 0.3562 - val_accuracy: 0.8642\n",
      "Epoch 21/50\n",
      "937/937 [==============================] - 21s 22ms/step - loss: 0.3500 - accuracy: 0.8674 - val_loss: 0.3494 - val_accuracy: 0.8637\n",
      "Epoch 22/50\n",
      "937/937 [==============================] - 21s 23ms/step - loss: 0.3538 - accuracy: 0.8676 - val_loss: 0.3421 - val_accuracy: 0.8718\n",
      "Epoch 23/50\n",
      "937/937 [==============================] - 20s 22ms/step - loss: 0.3572 - accuracy: 0.8652 - val_loss: 0.3343 - val_accuracy: 0.8754\n",
      "Epoch 24/50\n",
      "937/937 [==============================] - 23s 24ms/step - loss: 0.3549 - accuracy: 0.8658 - val_loss: 0.3392 - val_accuracy: 0.8722\n",
      "Epoch 25/50\n",
      "937/937 [==============================] - 21s 22ms/step - loss: 0.3467 - accuracy: 0.8695 - val_loss: 0.3390 - val_accuracy: 0.8725\n",
      "Epoch 26/50\n",
      "937/937 [==============================] - 21s 23ms/step - loss: 0.3374 - accuracy: 0.8720 - val_loss: 0.3441 - val_accuracy: 0.8709\n",
      "Epoch 27/50\n",
      "937/937 [==============================] - 21s 23ms/step - loss: 0.3281 - accuracy: 0.8763 - val_loss: 0.3122 - val_accuracy: 0.8816\n",
      "Epoch 28/50\n",
      "937/937 [==============================] - 21s 22ms/step - loss: 0.3201 - accuracy: 0.8802 - val_loss: 0.3143 - val_accuracy: 0.8841\n",
      "Epoch 29/50\n",
      "937/937 [==============================] - 22s 23ms/step - loss: 0.3088 - accuracy: 0.8844 - val_loss: 0.3045 - val_accuracy: 0.8868\n",
      "Epoch 30/50\n",
      "937/937 [==============================] - 21s 22ms/step - loss: 0.3029 - accuracy: 0.8873 - val_loss: 0.3054 - val_accuracy: 0.8861\n",
      "Epoch 31/50\n",
      "937/937 [==============================] - 21s 23ms/step - loss: 0.2959 - accuracy: 0.8900 - val_loss: 0.2960 - val_accuracy: 0.8902\n",
      "Epoch 32/50\n",
      "937/937 [==============================] - 25s 26ms/step - loss: 0.2909 - accuracy: 0.8919 - val_loss: 0.2925 - val_accuracy: 0.8909\n",
      "Epoch 33/50\n",
      "937/937 [==============================] - 21s 22ms/step - loss: 0.2866 - accuracy: 0.8938 - val_loss: 0.2961 - val_accuracy: 0.8890\n",
      "Epoch 34/50\n",
      "937/937 [==============================] - 23s 24ms/step - loss: 0.2926 - accuracy: 0.8898 - val_loss: 0.2974 - val_accuracy: 0.8889\n",
      "Epoch 35/50\n",
      "937/937 [==============================] - 26s 28ms/step - loss: 0.2984 - accuracy: 0.8881 - val_loss: 0.2982 - val_accuracy: 0.8886\n",
      "Epoch 36/50\n",
      "937/937 [==============================] - 22s 23ms/step - loss: 0.3005 - accuracy: 0.8870 - val_loss: 0.3188 - val_accuracy: 0.8794\n",
      "Epoch 37/50\n",
      "937/937 [==============================] - 21s 23ms/step - loss: 0.3064 - accuracy: 0.8841 - val_loss: 0.3095 - val_accuracy: 0.8856\n",
      "Epoch 38/50\n",
      "937/937 [==============================] - 24s 25ms/step - loss: 0.3103 - accuracy: 0.8817 - val_loss: 0.3197 - val_accuracy: 0.8789\n",
      "Epoch 39/50\n",
      "937/937 [==============================] - 22s 23ms/step - loss: 0.3156 - accuracy: 0.8829 - val_loss: 0.3091 - val_accuracy: 0.8859\n",
      "Epoch 40/50\n",
      "937/937 [==============================] - 29s 31ms/step - loss: 0.3182 - accuracy: 0.8799 - val_loss: 0.3053 - val_accuracy: 0.8847\n",
      "Epoch 41/50\n",
      "937/937 [==============================] - 23s 25ms/step - loss: 0.3177 - accuracy: 0.8811 - val_loss: 0.3134 - val_accuracy: 0.8830\n",
      "Epoch 42/50\n",
      "937/937 [==============================] - 22s 23ms/step - loss: 0.3079 - accuracy: 0.8832 - val_loss: 0.3073 - val_accuracy: 0.8835\n",
      "Epoch 43/50\n",
      "937/937 [==============================] - 21s 23ms/step - loss: 0.2997 - accuracy: 0.8887 - val_loss: 0.3185 - val_accuracy: 0.8792\n",
      "Epoch 44/50\n",
      "937/937 [==============================] - 21s 23ms/step - loss: 0.2893 - accuracy: 0.8909 - val_loss: 0.2912 - val_accuracy: 0.8908\n",
      "Epoch 45/50\n",
      "937/937 [==============================] - 22s 24ms/step - loss: 0.2826 - accuracy: 0.8949 - val_loss: 0.2959 - val_accuracy: 0.8885\n",
      "Epoch 46/50\n",
      "937/937 [==============================] - 20s 22ms/step - loss: 0.2764 - accuracy: 0.8967 - val_loss: 0.2875 - val_accuracy: 0.8921\n",
      "Epoch 47/50\n",
      "937/937 [==============================] - 22s 23ms/step - loss: 0.2714 - accuracy: 0.8982 - val_loss: 0.2786 - val_accuracy: 0.8967\n",
      "Epoch 48/50\n",
      "937/937 [==============================] - 21s 23ms/step - loss: 0.2666 - accuracy: 0.9005 - val_loss: 0.2737 - val_accuracy: 0.8999\n",
      "Epoch 49/50\n",
      "937/937 [==============================] - 21s 23ms/step - loss: 0.2645 - accuracy: 0.9019 - val_loss: 0.2765 - val_accuracy: 0.8964\n",
      "Epoch 50/50\n",
      "937/937 [==============================] - 22s 23ms/step - loss: 0.2684 - accuracy: 0.8999 - val_loss: 0.2795 - val_accuracy: 0.8962\n",
      "[INFO] evaluating network...\n",
      "157/157 [==============================] - 1s 4ms/step\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         top       0.84      0.85      0.84      1000\n",
      "     trouser       0.98      0.98      0.98      1000\n",
      "    pullover       0.84      0.82      0.83      1000\n",
      "       dress       0.88      0.92      0.90      1000\n",
      "        coat       0.85      0.79      0.82      1000\n",
      "      sandal       0.98      0.97      0.97      1000\n",
      "       shirt       0.70      0.73      0.72      1000\n",
      "     sneaker       0.94      0.97      0.96      1000\n",
      "         bag       0.98      0.98      0.98      1000\n",
      "  ankle boot       0.97      0.95      0.96      1000\n",
      "\n",
      "    accuracy                           0.90     10000\n",
      "   macro avg       0.90      0.90      0.90     10000\n",
      "weighted avg       0.90      0.90      0.90     10000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "stepSize = STEP_SIZE * (trainX.shape[0] // BATCH_SIZE)\n",
    "clr = CyclicLR(\n",
    "\tmode=CLR_METHOD,\n",
    "\tbase_lr=MIN_LR,\n",
    "\tmax_lr=MAX_LR,\n",
    "\tstep_size=stepSize)\n",
    "# train the network\n",
    "print(\"[INFO] training network...\")\n",
    "H = model.fit(\n",
    "\tx=aug.flow(trainX, trainY, batch_size=BATCH_SIZE),\n",
    "\tvalidation_data=(testX, testY),\n",
    "\tsteps_per_epoch=trainX.shape[0] // BATCH_SIZE,\n",
    "\tepochs=NUM_EPOCHS,\n",
    "\tcallbacks=[clr],\n",
    "\tverbose=1)\n",
    "# evaluate the network and show a classification report\n",
    "print(\"[INFO] evaluating network...\")\n",
    "predictions = model.predict(x=testX, batch_size=BATCH_SIZE)\n",
    "print(classification_report(testY.argmax(axis=1),\n",
    "\tpredictions.argmax(axis=1), target_names=CLASSES))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "N = np.arange(0, NUM_EPOCHS)\n",
    "plt.style.use(\"ggplot\")\n",
    "plt.figure()\n",
    "plt.plot(N, H.history[\"loss\"], label=\"train_loss\")\n",
    "plt.plot(N, H.history[\"val_loss\"], label=\"val_loss\")\n",
    "plt.plot(N, H.history[\"accuracy\"], label=\"train_acc\")\n",
    "plt.plot(N, H.history[\"val_accuracy\"], label=\"val_acc\")\n",
    "plt.title(\"Training Loss and Accuracy\")\n",
    "plt.xlabel(\"Epoch #\")\n",
    "plt.ylabel(\"Loss/Accuracy\")\n",
    "plt.legend(loc=\"lower left\")\n",
    "plt.savefig(\"training_plot.png\")\n",
    "# plot the learning rate history\n",
    "N = np.arange(0, len(clr.history[\"lr\"]))\n",
    "plt.figure()\n",
    "plt.plot(N, clr.history[\"lr\"])\n",
    "plt.title(\"Cyclical Learning Rate (CLR)\")\n",
    "plt.xlabel(\"Training Iterations\")\n",
    "plt.ylabel(\"Learning Rate\")\n",
    "plt.savefig(\"crl_plot.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Use the cyclical learning rate policy (with exponential decay) and train your network using batch size 64 and lrmin and lrmax values obtained in part 1. Plot train/validation loss and accuracy curve (similar to Figure 4 in reference)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. We want to test if increasing batch size for a fixed learning rate has the same effect as decreasing learning rate for a fixed batch size. Fix learning rate to lrmax and train your network starting with batch size 32 and incrementally going upto 4096 (in increments of a factor of 2; like 32, 64...). You can choose a step size (in terms of number of epochs) to increment the batch size. Plot the training loss vs. log2(batch size). Is the generalization of your final model similar or different than cyclical learning rate policy?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.5 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "2a0f08725565d2d00e70b9fae99b105541271f7be4baa13607a6e77e1d2c8c73"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
