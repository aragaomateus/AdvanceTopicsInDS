{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CSI_3d7mzIa2"
      },
      "source": [
        "# Problem 6 - Sentiment Analysis using recurrent models 20 points\n",
        "\n",
        "In this problem, you will compare the performance of RNN, LSTM, GRU and BiLSTM for the task of sentiment analysis. You’ll use the IMDB sentiment analysis dataset for this task - Sentiment Analysis of IMDB Movie Reviews. For each model, use a single cell, and keep the number of units fixed to 256. Train each model for 10 epochs using the Adam optimizer, batch size of 256, and a learning rate of 0.01."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "e1bldC9Jzv56"
      },
      "outputs": [],
      "source": [
        "from spacy.tokenizer import Tokenizer\n",
        "from spacy.lang.en import English\n",
        "\n",
        "def process_tokens(text):\n",
        "    \"\"\"\n",
        "    function to process tokens, replace any unwanted chars\n",
        "    \"\"\"\n",
        "    preprocessed_text = text.lower().replace(\",\", \"\").replace(\".\", \"\").replace(\":\", \"\").replace(\")\", \"\").replace(\"-\", \"\").replace(\"(\", \"\")\n",
        "    preprocessed_text = ''.join([i for i in preprocessed_text if not preprocessed_text.isdigit()])\n",
        "    return preprocessed_text\n",
        "\n",
        "def preprocessing(data):\n",
        "    \"\"\"\n",
        "    preprocessing data to list of tokens\n",
        "    \"\"\"\n",
        "    nlp = English()\n",
        "    tokenizer = Tokenizer(nlp.vocab)\n",
        "    preprocessed_data = []\n",
        "    for sentence in data:\n",
        "        sentence = process_tokens(sentence)\n",
        "        tokens = tokenizer(sentence)\n",
        "        tlist = []\n",
        "        for token in tokens:\n",
        "            tlist.append(str(token))\n",
        "        preprocessed_data.append(tlist)\n",
        "    return preprocessed_data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "icpiwefjzIa4"
      },
      "source": [
        "1. Import the dataset and convert it into vector form using Bag of Words technique.(2) "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "18O05bc9zIa5",
        "outputId": "10d74743-bd40-4ff5-f6ea-589ff88f1bfe"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1o5Hu9mOZsXxhIbPEov80LsxxCs5A2_7W\n",
            "To: /content/imdb.csv\n",
            "100% 66.2M/66.2M [00:02<00:00, 32.8MB/s]\n"
          ]
        }
      ],
      "source": [
        "!gdown 1o5Hu9mOZsXxhIbPEov80LsxxCs5A2_7W -O './imdb.csv'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6k6pKOyLzZfd",
        "outputId": "b86baef2-06ff-472a-8f9e-f689ff0e4b33"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "35000 7499 7499\n",
            "35000 35000 7499\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "df = pd.read_csv(\"imdb.csv\", usecols=[\"review\", \"sentiment\"], encoding='latin-1')\n",
        "## 1 - positive, 0 - negative\n",
        "df.sentiment = (df.sentiment == \"positive\").astype(\"int\")\n",
        "df.head()\n",
        "\n",
        "val_size = int(df.shape[0] * 0.15)\n",
        "test_size = int(df.shape[0] * 0.15)\n",
        "\n",
        "\n",
        "def train_val_test_split(df=None, train_percent=0.7, test_percent=0.15, val_percent=0.15):\n",
        "  df = df.sample(frac=1)\n",
        "  train_df = df[: int(len(df)*train_percent)]\n",
        "  test_df = df[int(len(df)*train_percent)+1 : int(len(df)*(train_percent+test_percent))]\n",
        "  val_df = df[int(len(df)*(train_percent + test_percent))+1 : ]\n",
        "  return train_df, test_df, val_df\n",
        "\n",
        "train_df, test_df, val_df = train_val_test_split(df, 0.7, 0.15, 0.15)\n",
        "train_labels, train_texts = train_df.values[:,1], train_df.values[:,0]\n",
        "val_labels, val_texts = val_df.values[:,1], val_df.values[:,0]\n",
        "test_labels, test_texts = test_df.values[:,1], test_df.values[:,0]\n",
        "print(len(train_df), len(test_df), len(val_df))\n",
        "print(len(train_texts), len(train_labels), len(val_df))\n",
        "\n",
        "\n",
        "train_data = preprocessing(train_texts)\n",
        "val_data = preprocessing(val_texts)\n",
        "test_data = preprocessing(test_texts)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "Zp4x42NC1Omt"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import itertools\n",
        "\n",
        "## Creating a vectorizer to vectorize text and create matrix of features\n",
        "## Bag of words technique\n",
        "class Vectorizer():\n",
        "    def __init__(self, max_features):\n",
        "        self.max_features = max_features\n",
        "        self.vocab_list = None\n",
        "        self.token_to_index = None\n",
        "\n",
        "    def fit(self, dataset):\n",
        "        word_dict = {}\n",
        "        for sentence in dataset:\n",
        "            for token in sentence:\n",
        "                if token not in word_dict:\n",
        "                    word_dict[token] = 1\n",
        "                else:\n",
        "                    word_dict[token] += 1\n",
        "        word_dict = dict(sorted(word_dict.items(), key=lambda item: item[1], reverse=True))\n",
        "        end_to_slice = min(len(word_dict), self.max_features)\n",
        "        word_dict = dict(itertools.islice(word_dict.items(), end_to_slice))\n",
        "        self.vocab_list = list(word_dict.keys())\n",
        "        self.token_to_index = {}\n",
        "        counter = 0\n",
        "        for token in self.vocab_list:\n",
        "            self.token_to_index[token] = counter\n",
        "            counter += 1\n",
        "\n",
        "\n",
        "    def transform(self, dataset):\n",
        "        data_matrix = np.zeros((len(dataset), len(self.vocab_list)))\n",
        "        for i, sentence in enumerate(dataset):\n",
        "            for token in sentence:\n",
        "                if token in self.token_to_index:\n",
        "                    data_matrix[i, self.token_to_index[token]] += 1\n",
        "        return data_matrix\n",
        "\n",
        "## max features - top k words to consider only\n",
        "max_features = 2000 \n",
        "\n",
        "vectorizer = Vectorizer(max_features=max_features)\n",
        "vectorizer.fit(train_data)\n",
        "\n",
        "## Checking if the len of vocab = k \n",
        "X_train = vectorizer.transform(train_data)\n",
        "X_val = vectorizer.transform(val_data)\n",
        "X_test = vectorizer.transform(test_data)\n",
        "\n",
        "y_train = np.array(train_labels)\n",
        "y_val = np.array(val_labels)\n",
        "y_test = np.array(test_labels)\n",
        "\n",
        "vocab = vectorizer.vocab_list"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wGK7JV7r2OGE"
      },
      "outputs": [],
      "source": [
        "vocab"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SFW5eX5PzIa7"
      },
      "source": [
        "2. Define an RNN model and train it on the dataset (4)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ln_77zLK2QSj",
        "outputId": "d9d15138-4f89-443c-c0b4-711674bd31f9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "X_train.shape: (35000, 1, 2000), y_train.shape: (35000, 2)\n"
          ]
        }
      ],
      "source": [
        "from tensorflow.keras.utils import to_categorical\n",
        "\n",
        "y_train = y_train.astype('int')\n",
        "y_val = y_val.astype('int')\n",
        "y_test = y_test.astype('int')\n",
        "\n",
        "y_train = to_categorical(y_train, 2)\n",
        "y_test = to_categorical(y_test, 2)\n",
        "y_val = to_categorical(y_val, 2)\n",
        "\n",
        "X_train = X_train.reshape(-1, 1, X_train.shape[1])\n",
        "X_val = X_val.reshape(-1, 1, X_val.shape[1])\n",
        "X_test = X_test.reshape(-1, 1, X_test.shape[1])\n",
        "\n",
        "y_train = y_train.reshape(-1, 2)\n",
        "y_val = y_val.reshape(-1, 2)\n",
        "y_test = y_test.reshape(-1, 2)\n",
        "\n",
        "print(f'X_train.shape: {X_train.shape}, y_train.shape: {y_train.shape}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "G_cjAjlz2mcE",
        "outputId": "969ae63e-95a3-4d0a-c2de-d072f20d3d90"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model: \"sequential_10\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " simple_rnn_5 (SimpleRNN)    (None, 128)               272512    \n",
            "                                                                 \n",
            " dense_10 (Dense)            (None, 2)                 258       \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 272,770\n",
            "Trainable params: 272,770\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "None\n",
            "Epoch 1/10\n",
            "137/137 [==============================] - 2s 10ms/step - loss: 0.3651 - accuracy: 0.8378 - val_loss: 0.2882 - val_accuracy: 0.8813\n",
            "Epoch 2/10\n",
            "137/137 [==============================] - 1s 8ms/step - loss: 0.2696 - accuracy: 0.8896 - val_loss: 0.2863 - val_accuracy: 0.8789\n",
            "Epoch 3/10\n",
            "137/137 [==============================] - 1s 8ms/step - loss: 0.2510 - accuracy: 0.8963 - val_loss: 0.2906 - val_accuracy: 0.8797\n",
            "Epoch 4/10\n",
            "137/137 [==============================] - 1s 8ms/step - loss: 0.2383 - accuracy: 0.9038 - val_loss: 0.2944 - val_accuracy: 0.8767\n",
            "Epoch 5/10\n",
            "137/137 [==============================] - 1s 8ms/step - loss: 0.2198 - accuracy: 0.9124 - val_loss: 0.2981 - val_accuracy: 0.8756\n",
            "Epoch 6/10\n",
            "137/137 [==============================] - 1s 8ms/step - loss: 0.2096 - accuracy: 0.9166 - val_loss: 0.3001 - val_accuracy: 0.8767\n",
            "Epoch 7/10\n",
            "137/137 [==============================] - 1s 8ms/step - loss: 0.1922 - accuracy: 0.9253 - val_loss: 0.3069 - val_accuracy: 0.8734\n",
            "Epoch 8/10\n",
            "137/137 [==============================] - 1s 8ms/step - loss: 0.1749 - accuracy: 0.9343 - val_loss: 0.3147 - val_accuracy: 0.8718\n",
            "Epoch 9/10\n",
            "137/137 [==============================] - 1s 8ms/step - loss: 0.1570 - accuracy: 0.9438 - val_loss: 0.3205 - val_accuracy: 0.8748\n",
            "Epoch 10/10\n",
            "137/137 [==============================] - 1s 8ms/step - loss: 0.1381 - accuracy: 0.9518 - val_loss: 0.3276 - val_accuracy: 0.8734\n",
            "dict_keys(['loss', 'accuracy', 'val_loss', 'val_accuracy'])\n"
          ]
        }
      ],
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense\n",
        "from tensorflow.keras.layers import SimpleRNN, Dropout\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "\n",
        "model = None\n",
        "model = Sequential()\n",
        "model.add(SimpleRNN(128, input_shape=(1, max_features)))\n",
        "model.add(Dense(2, activation='softmax'))\n",
        "\n",
        "optimizer = Adam()\n",
        "model.compile(loss='categorical_crossentropy', optimizer=optimizer,\n",
        "              metrics=['accuracy'])\n",
        "print(model.summary())\n",
        "history = model.fit(X_train, y_train,\n",
        "          batch_size=256,\n",
        "          validation_data=(X_val, y_val),\n",
        "          epochs=10)\n",
        "print(history.history.keys())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "n__fE2zb2s6a",
        "outputId": "e89c3ac2-09a1-4440-f41e-10fa5cb1de54"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Test loss: 0.34322747588157654\n",
            "Test accuracy: 0.8715828657150269\n"
          ]
        }
      ],
      "source": [
        "score, acc = model.evaluate(X_test, y_test, verbose=0)\n",
        "print('Test loss:', score)\n",
        "print('Test accuracy:', acc)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6up3bJVOzIa8"
      },
      "source": [
        "3. Define a LSTM model and train it on the dataset (4)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "328GmmaY4qSK",
        "outputId": "0b91e9be-3088-45cf-c6ba-41ee614a80ce"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model: \"sequential_11\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " lstm_5 (LSTM)               (None, 128)               1090048   \n",
            "                                                                 \n",
            " dense_11 (Dense)            (None, 2)                 258       \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 1,090,306\n",
            "Trainable params: 1,090,306\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "None\n",
            "Epoch 1/10\n",
            "137/137 [==============================] - 5s 28ms/step - loss: 0.3614 - accuracy: 0.8454 - val_loss: 0.2905 - val_accuracy: 0.8792\n",
            "Epoch 2/10\n",
            "137/137 [==============================] - 3s 23ms/step - loss: 0.2613 - accuracy: 0.8927 - val_loss: 0.2959 - val_accuracy: 0.8767\n",
            "Epoch 3/10\n",
            "137/137 [==============================] - 3s 23ms/step - loss: 0.2235 - accuracy: 0.9099 - val_loss: 0.2928 - val_accuracy: 0.8765\n",
            "Epoch 4/10\n",
            "137/137 [==============================] - 3s 22ms/step - loss: 0.1909 - accuracy: 0.9258 - val_loss: 0.2933 - val_accuracy: 0.8788\n",
            "Epoch 5/10\n",
            "137/137 [==============================] - 3s 22ms/step - loss: 0.1546 - accuracy: 0.9432 - val_loss: 0.3123 - val_accuracy: 0.8744\n",
            "Epoch 6/10\n",
            "137/137 [==============================] - 3s 22ms/step - loss: 0.1186 - accuracy: 0.9602 - val_loss: 0.3279 - val_accuracy: 0.8765\n",
            "Epoch 7/10\n",
            "137/137 [==============================] - 3s 22ms/step - loss: 0.0852 - accuracy: 0.9760 - val_loss: 0.3423 - val_accuracy: 0.8767\n",
            "Epoch 8/10\n",
            "137/137 [==============================] - 3s 22ms/step - loss: 0.0593 - accuracy: 0.9860 - val_loss: 0.3640 - val_accuracy: 0.8751\n",
            "Epoch 9/10\n",
            "137/137 [==============================] - 3s 22ms/step - loss: 0.0412 - accuracy: 0.9923 - val_loss: 0.3821 - val_accuracy: 0.8772\n",
            "Epoch 10/10\n",
            "137/137 [==============================] - 3s 22ms/step - loss: 0.0279 - accuracy: 0.9957 - val_loss: 0.4091 - val_accuracy: 0.8761\n",
            "dict_keys(['loss', 'accuracy', 'val_loss', 'val_accuracy'])\n"
          ]
        }
      ],
      "source": [
        "from tensorflow.keras.layers import LSTM\n",
        "\n",
        "model = None\n",
        "model = Sequential()\n",
        "model.add(LSTM(128, input_shape=(1, max_features)))\n",
        "model.add(Dense(2, activation='softmax'))\n",
        "\n",
        "optimizer = Adam()\n",
        "model.compile(loss='categorical_crossentropy', optimizer=optimizer,\n",
        "              metrics=['accuracy'])\n",
        "print(model.summary())\n",
        "history = model.fit(X_train, y_train,\n",
        "          batch_size=256,\n",
        "          validation_data=(X_val, y_val),\n",
        "          epochs=10)\n",
        "print(history.history.keys())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TvrRBZec471R",
        "outputId": "9594d6fe-e055-4d97-c93a-e0f7c75b3163"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Test loss: 0.4387073218822479\n",
            "Test accuracy: 0.8751833438873291\n"
          ]
        }
      ],
      "source": [
        "score, acc = model.evaluate(X_test, y_test, verbose=0)\n",
        "print('Test loss:', score)\n",
        "print('Test accuracy:', acc)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VbmeltjCzIa8"
      },
      "source": [
        "4. Define a GRU model and train it on the dataset (4)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "A560lYj_5DBe",
        "outputId": "acf14b13-7ce9-4f40-eacf-efd16ce64ca6"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model: \"sequential_12\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " gru_1 (GRU)                 (None, 128)               817920    \n",
            "                                                                 \n",
            " dense_12 (Dense)            (None, 2)                 258       \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 818,178\n",
            "Trainable params: 818,178\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "None\n",
            "Epoch 1/10\n",
            "137/137 [==============================] - 5s 20ms/step - loss: 0.3542 - accuracy: 0.8476 - val_loss: 0.2924 - val_accuracy: 0.8795\n",
            "Epoch 2/10\n",
            "137/137 [==============================] - 2s 17ms/step - loss: 0.2587 - accuracy: 0.8946 - val_loss: 0.3112 - val_accuracy: 0.8692\n",
            "Epoch 3/10\n",
            "137/137 [==============================] - 2s 18ms/step - loss: 0.2311 - accuracy: 0.9069 - val_loss: 0.2892 - val_accuracy: 0.8779\n",
            "Epoch 4/10\n",
            "137/137 [==============================] - 2s 17ms/step - loss: 0.2020 - accuracy: 0.9200 - val_loss: 0.2975 - val_accuracy: 0.8771\n",
            "Epoch 5/10\n",
            "137/137 [==============================] - 2s 17ms/step - loss: 0.1712 - accuracy: 0.9342 - val_loss: 0.3044 - val_accuracy: 0.8771\n",
            "Epoch 6/10\n",
            "137/137 [==============================] - 2s 18ms/step - loss: 0.1402 - accuracy: 0.9495 - val_loss: 0.3187 - val_accuracy: 0.8771\n",
            "Epoch 7/10\n",
            "137/137 [==============================] - 2s 18ms/step - loss: 0.1102 - accuracy: 0.9629 - val_loss: 0.3327 - val_accuracy: 0.8746\n",
            "Epoch 8/10\n",
            "137/137 [==============================] - 2s 18ms/step - loss: 0.0817 - accuracy: 0.9753 - val_loss: 0.3558 - val_accuracy: 0.8755\n",
            "Epoch 9/10\n",
            "137/137 [==============================] - 2s 18ms/step - loss: 0.0598 - accuracy: 0.9840 - val_loss: 0.3772 - val_accuracy: 0.8714\n",
            "Epoch 10/10\n",
            "137/137 [==============================] - 3s 21ms/step - loss: 0.0422 - accuracy: 0.9913 - val_loss: 0.4027 - val_accuracy: 0.8738\n",
            "dict_keys(['loss', 'accuracy', 'val_loss', 'val_accuracy'])\n",
            "Test loss: 0.44197070598602295\n",
            "Test accuracy: 0.8702493906021118\n"
          ]
        }
      ],
      "source": [
        "from tensorflow.keras.layers import GRU\n",
        "\n",
        "model = None\n",
        "model = Sequential()\n",
        "model.add(GRU(128, input_shape=(1, max_features)))\n",
        "model.add(Dense(2, activation='softmax'))\n",
        "\n",
        "optimizer = Adam()\n",
        "model.compile(loss='categorical_crossentropy', optimizer=optimizer,\n",
        "              metrics=['accuracy'])\n",
        "print(model.summary())\n",
        "history = model.fit(X_train, y_train,\n",
        "          batch_size=256,\n",
        "          validation_data=(X_val, y_val),\n",
        "          epochs=10)\n",
        "print(history.history.keys())\n",
        "\n",
        "score, acc = model.evaluate(X_test, y_test, verbose=0)\n",
        "print('Test loss:', score)\n",
        "print('Test accuracy:', acc)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9XtLmyx87W9M",
        "outputId": "62df56d3-58db-4bad-e2cf-9b578f2ec2a1"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "235/235 [==============================] - 1s 2ms/step\n",
            "Label predicted: 1, Actual label: 1\n",
            "text: Love Jones cleverly portrays young African-American men and women in a clear, positive, realistic sense. I feel that all of the actors and actresses were magnificent and really did a great job at capturing the mood. Nia Long and Larenz Tate worked well together and I hope to see more work from the two of them. As a matter of fact all of the actors/actresses did such a fine job it would be great to see another romantic-comedy from them. This movie can be compared to most any well-written, romantic comedy. If you have not seen this movie already I strongly recommend that you do, it can definitely give you another perspective on life and love.\n",
            "Label predicted: 0, Actual label: 0\n",
            "text: This version is very painful to watch. All of the acting is very stilted but especially that of Norma Shearer who is still acting as though she were in a silent movie instead of a talkie. Check out the 1937 version with Joan Crawford, Robert Montgomery and William Powell which is much more entertaining.\n",
            "Label predicted: 0, Actual label: 0\n",
            "text: An EXTREMELY fast paced,exhilarating, interesting, detail rich book. Its a huge shame that the film had none of these qualities. not only was Tom Hanks' mild mannered portrayal or Robert Langdon Laughable, but the name changes to key characters, huge deviances from the original story line, and poor Irish/Italian accent from Carmalengo Played by Ewan Mcgregor, made for the worst book to film EVER.<br /><br />As a huge fan of A&D the book, i had high hopes for a more lavish, true to book detailed movie, where it would start and finish just as the book did - leaving me wanting more.<br /><br />All the film really did was depress me within the 1st 10 minutes.<br /><br />what was impressive was how the... sorry! i couldn't even finish that sentence without laughing.<br /><br />in short - Vittoria was the token hottie, a very second to Audrey Tatou and there were some very nice Alfa Romeos.<br /><br />i would recommend reading the book to understand that, if Ron Howard must insist on making ANOTHER book to film, i would be happy saving my Â£6.40 for a KFC zinger meal and some chicken wings - far more entertaining and deeply more satisfying!\n",
            "Label predicted: 1, Actual label: 1\n",
            "text: Beforehand Notification: I'm sure someone is going to accuse me of playing the race card here, but when I saw the preview for this movie, I was thinking \"Finally!\" I have yet to see one movie about popular African-influenced dance (be it popular hip hop moves, breaking, or stepping) where the main character was a Black woman. I've seen an excessive amount of movies where a non-Black woman who knew nothing about hip hop comes fresh to the hood and does a mediocre job of it (Breakin, Breakin 2, Save the Last Dance, Step Up), but the Black women in the film are almost nonexistent. That always bothered me considering so much of hip hop, African-influenced dance, and breaking was with Blacks and Latinos in massive amounts in these particular sets and it wasn't always men who performed it, so I felt this movie has been a long time coming. However, the race does not make the film, so I also wanted it to carry a believable plot; the dancing be entertaining; and interesting to watch.<br /><br />Pros: I really enjoyed this film bringing Jamaican culture. I can't recall ever seeing a popular, mainstream film where all the main characters were Jamaican; had believable accents; and weren't stereotypical with the beanies. The steppers, family, friends, and even the \"thugs\" were all really intelligent, realistic people who were trying to love, live, and survive in the neighborhood they lived in by doing something positive. Even when the audience was made aware that the main character's sister chose an alternate lifestyle, it still didn't make the plot stereotypical. I was satisfied with the way it was portrayed. I LOVED the stepping; the romantic flirty relationship going on between two steppers; the trials that the main character's parents were going through; and how she dealt with coming back to her old neighborhood and dealing with Crabs in a Barrel. I respected that she was so intelligent and active at the same time, and so many other sistas in the film were handling themselves in the step world. They were all just as excellent as the fellas. I don't see that in too many movies nowadays, at least not those that would be considered Black films.<br /><br />Cons: I'm not quite sure why the directors or whoever put the movie together did this, but I question whether they've been to real step shows. Whenever the steppers got ready to perform, some hip hop song would play in place of the steppers' hand/feet beats. At a real step show, there is zero need for music, other than to maybe entertain the crowds in between groups. And then when hip hop songs were played, sometimes the beat to the song was off to the beat of the steppers' hands and feet. It was awkward. I was more impressed with the stepping in this movie versus \"Stomp the Yard\" (another great stepping movie) because the women got to represent as fierce as the guys (in \"Stomp the Yard,\" Meagan Good got all of a few seconds of some prissy twirl and hair flip and the (Deltas?) let out a chant and a few steps and were cut immediately). Even when there were very small scenes, the ladies tore it up, especially in the auto shop, and it was without all that music to drown out their physical music. I know soundtracks have to be sold, but the movie folks could've played the music in other parts of the film.<br /><br />I'm not a Keyshia Cole fan, so every time I saw her, all I kept thinking was \"Is it written in the script for her to constantly put her hand on her hip when she talks?\" She looked uncomfortable on screen to me. I thought they should've used a host like Free or Rocsi instead. Deray Davis was funny as usual though. Also, I groaned when I found out that the movie was supposed to be in the ghetto, like stepping couldn't possibly happen anywhere else. Hollywood, as usual. However, only a couple of people were portrayed as excessively ignorant due to their neighborhood and losers, which mainstream movies tend to do.<br /><br />I would've given this movie five stars, but the music playing killed it for me. I definitely plan to buy it when it comes out and hopefully the bonus scenes will include the actual step shows without all the songs.\n",
            "Label predicted: 1, Actual label: 1\n",
            "text: \"Atlantis: The Lost Empire\" was everything the previews indicated it would be. It is not often you find that. Most of the time, the previews show only the best parts and then the rest of the movie is terrible. Not so with this one. I was pleased with the original plot, even though the sub-plots were not. The animation was not break through like \"Shrek\" but it was good, none the less. The plot and the story line were well presented and there were only a few slow spots in them. This keeps you interested. I found myself enjoying this one. \"Atlantis\" gets and keeps your attention. You also have to think a little bit, but not too much. Once you think about it a little, you can figure out what needs to happen but you really don't know for sure how it is going to happen.<br /><br />The casting was also good. Michael J. Fox, as Milo was an excellent choice. His personality fits nicely. The gruff natured Commander Rourke was also well chosen with James Garner. His character reminded me of his performance in \"Maverick\" which I also liked. I really liked the casting of Claudia Christian as Helga Sinclair. Her ability to play a no nonsense personality makes the film more interesting. It's just too bad she is a villain.<br /><br />Over all, definitely worth you while (8 out of 10).\n"
          ]
        }
      ],
      "source": [
        "# check predictions\n",
        "from tensorflow.keras.backend import argmax\n",
        "\n",
        "y_pred = model.predict(X_test)\n",
        "for i in range(5):\n",
        "  print(f'Label predicted: {argmax(y_pred[i]).numpy()}, Actual label: {argmax(y_test[i]).numpy()}')\n",
        "  print(f'text: {test_texts[i]}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Tvq6L-kUzIa8"
      },
      "source": [
        "5. Define a BiLSTM model and train it on the dataset (4)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XihI4o_FzIa9",
        "outputId": "d4633f48-bda1-42e2-d17a-0a9a83e04c32"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model: \"sequential_7\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " bidirectional (Bidirectiona  (None, 1, 512)           4622336   \n",
            " l)                                                              \n",
            "                                                                 \n",
            " bidirectional_1 (Bidirectio  (None, 512)              1574912   \n",
            " nal)                                                            \n",
            "                                                                 \n",
            " dense_7 (Dense)             (None, 2)                 1026      \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 6,198,274\n",
            "Trainable params: 6,198,274\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "None\n",
            "Epoch 1/10\n",
            "137/137 [==============================] - 24s 140ms/step - loss: 0.5976 - accuracy: 0.7325 - val_loss: 0.4370 - val_accuracy: 0.8644\n",
            "Epoch 2/10\n",
            "137/137 [==============================] - 18s 129ms/step - loss: 0.3787 - accuracy: 0.8784 - val_loss: 0.3599 - val_accuracy: 0.8684\n",
            "Epoch 3/10\n",
            "137/137 [==============================] - 19s 137ms/step - loss: 0.2935 - accuracy: 0.8982 - val_loss: 0.3275 - val_accuracy: 0.8661\n",
            "Epoch 4/10\n",
            "137/137 [==============================] - 18s 134ms/step - loss: 0.2489 - accuracy: 0.9126 - val_loss: 0.3137 - val_accuracy: 0.8765\n",
            "Epoch 5/10\n",
            "137/137 [==============================] - 17s 128ms/step - loss: 0.2046 - accuracy: 0.9326 - val_loss: 0.3347 - val_accuracy: 0.8726\n",
            "Epoch 6/10\n",
            "137/137 [==============================] - 18s 128ms/step - loss: 0.1666 - accuracy: 0.9464 - val_loss: 0.3510 - val_accuracy: 0.8617\n",
            "Epoch 7/10\n",
            "137/137 [==============================] - 18s 128ms/step - loss: 0.1275 - accuracy: 0.9632 - val_loss: 0.3588 - val_accuracy: 0.8741\n",
            "Epoch 8/10\n",
            "137/137 [==============================] - 18s 128ms/step - loss: 0.0937 - accuracy: 0.9760 - val_loss: 0.3863 - val_accuracy: 0.8705\n",
            "Epoch 9/10\n",
            "137/137 [==============================] - 18s 128ms/step - loss: 0.0726 - accuracy: 0.9834 - val_loss: 0.4315 - val_accuracy: 0.8618\n",
            "Epoch 10/10\n",
            "137/137 [==============================] - 18s 131ms/step - loss: 0.0580 - accuracy: 0.9872 - val_loss: 0.4358 - val_accuracy: 0.8681\n",
            "dict_keys(['loss', 'accuracy', 'val_loss', 'val_accuracy'])\n"
          ]
        }
      ],
      "source": [
        "from tensorflow.keras.layers import Bidirectional\n",
        "\n",
        "model = None\n",
        "model = Sequential()\n",
        "model.add(Bidirectional(LSTM(256, return_sequences=True), input_shape=(1, max_features)))\n",
        "model.add(Bidirectional(LSTM(256)))\n",
        "model.add(Dense(2, activation='softmax'))\n",
        "\n",
        "optimizer = Adam(learning_rate=0.01)\n",
        "model.compile(loss='categorical_crossentropy', optimizer=optimizer,\n",
        "              metrics=['accuracy'])\n",
        "print(model.summary())\n",
        "history = model.fit(X_train, y_train,\n",
        "          batch_size=256,\n",
        "          validation_data=(X_val, y_val),\n",
        "          epochs=10)\n",
        "print(history.history.keys())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sbqbWWSJ8wH2",
        "outputId": "904a430e-592a-4957-99e8-0656027505a2"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Test loss: 0.3505217730998993\n",
            "Test accuracy: 0.8695825934410095\n"
          ]
        }
      ],
      "source": [
        "score, acc = model.evaluate(X_test, y_test, verbose=0)\n",
        "print('Test loss:', score)\n",
        "print('Test accuracy:', acc)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HQ39QmrrzIa9"
      },
      "source": [
        "\n",
        "6. Compare the performance of all the models. In which case do you get the best accuracy? (2)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Simple RNN:\n",
        "Test loss: 0.34322747588157654\n",
        "Test accuracy: 0.8715828657150269\n",
        "\n",
        "LSTM:\n",
        "Test loss: 0.4387073218822479\n",
        "Test accuracy: 0.8751833438873291\n",
        "\n",
        "GRU:\n",
        "Test loss: 0.44197070598602295\n",
        "Test accuracy: 0.8702493906021118\n",
        "\n",
        "Bi LSTM\n",
        "Test loss: 0.3505217730998993\n",
        "Test accuracy: 0.8695825934410095\n",
        "\n",
        "Best Accuracy was given by the simple RNN model. "
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3.8.5 ('base')",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.5 (default, Sep  4 2020, 02:22:02) \n[Clang 10.0.0 ]"
    },
    "orig_nbformat": 4,
    "vscode": {
      "interpreter": {
        "hash": "2a0f08725565d2d00e70b9fae99b105541271f7be4baa13607a6e77e1d2c8c73"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
