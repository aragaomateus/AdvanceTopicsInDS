{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem 5 - Weight Initialization, Dead Neurons, Leaky ReLU\n",
    "\n",
    "Read the two blogs, one by Andre Pernunicic and other by Daniel Godoy on weight initialization. You will reuse the code at github repo linked in the blog for explaining vanishing and exploding gradients. You can use the same 5 layer neural network model as in the repo and the same dataset.\n",
    "\n",
    "• Andre Perunicic. Understand neural network weight initialization. Available at https://intoli.com/blog/neural-network-initialization/\n",
    "• Daniel Godoy. Hyper-parameters in Action Part II — Weight Initializers.\n",
    "• Initializers - Keras documentation. https://keras.io/initializers/.\n",
    "• Lu Lu et al. Dying ReLU and Initialization: Theory and Numerical Examples ."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. \n",
    "Explain vanishing gradients phenomenon using standard normalization with different values of standard deviation as given in the reference. Train the model with tanh and sigmoid activation functions. Next, show how Xavier (aka Glorot normal) initialization of weights helps in dealing with this problem. You should plot the gradients at each of the 5 layers for all 4 experiments to answer this question."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Explain vanishing gradients phenomenon using standard normalization with different values of standard deviation as given in the reference."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Normal initializer and sigmoid\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "in the google collab \n",
    "put the pictures in here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Normal initializer and tanh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Glorot and sigmoid "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Glorot and tanh"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2.\n",
    " The dying ReLU is a kind of vanishing gradient, which refers to a problem when ReLU neurons become\n",
    "inactive and only output 0 for any input. In the worst case of dying ReLU, ReLU neurons at a certain\n",
    "layer are all dead, i.e., the entire network dies and is referred as the dying ReLU neural networks in\n",
    "Lu et al (reference below). A dying ReLU neural network collapses to a constant function. Show this\n",
    "phenomenon using any one of the three 1-dimensional functions in page 13 of Lu et al. Use a ReLU\n",
    "network with 10 hidden layers, each of width 2 (hidden units per layer). Use minibatch of 64 and draw\n",
    "training data uniformly from [sqrt(− 7),sqrt() 7]. Perform 1000 independent training simulations each with\n",
    "3,000 training points. Out of these 1000 simulations, what fraction resulted in neural network collapse. Is your answer close to over 90% as was reported in Lu et al.? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "47/47 [==============================] - 1s 1ms/step - loss: 1.3831\n",
      "Epoch 2/10\n",
      "47/47 [==============================] - 0s 1ms/step - loss: 0.6992\n",
      "Epoch 3/10\n",
      "47/47 [==============================] - 0s 1ms/step - loss: 0.5982\n",
      "Epoch 4/10\n",
      "47/47 [==============================] - 0s 1ms/step - loss: 0.5829\n",
      "Epoch 5/10\n",
      "47/47 [==============================] - 0s 1ms/step - loss: 0.5804\n",
      "Epoch 6/10\n",
      "47/47 [==============================] - 0s 1ms/step - loss: 0.5801\n",
      "Epoch 7/10\n",
      "47/47 [==============================] - 0s 1ms/step - loss: 0.5801\n",
      "Epoch 8/10\n",
      "47/47 [==============================] - 0s 1ms/step - loss: 0.5801\n",
      "Epoch 9/10\n",
      "47/47 [==============================] - 0s 1ms/step - loss: 0.5801\n",
      "Epoch 10/10\n",
      "47/47 [==============================] - 0s 2ms/step - loss: 0.5802\n",
      "16/16 [==============================] - 0s 851us/step\n",
      "<class 'numpy.ndarray'>\n",
      "Epoch 1/10\n",
      "47/47 [==============================] - 1s 1ms/step - loss: 1.3906\n",
      "Epoch 2/10\n",
      "47/47 [==============================] - 0s 1ms/step - loss: 0.7021\n",
      "Epoch 3/10\n",
      "47/47 [==============================] - 0s 1ms/step - loss: 0.5987\n",
      "Epoch 4/10\n",
      "47/47 [==============================] - 0s 1ms/step - loss: 0.5830\n",
      "Epoch 5/10\n",
      "47/47 [==============================] - 0s 1ms/step - loss: 0.5804\n",
      "Epoch 6/10\n",
      "47/47 [==============================] - 0s 1ms/step - loss: 0.5801\n",
      "Epoch 7/10\n",
      "47/47 [==============================] - 0s 1ms/step - loss: 0.5801\n",
      "Epoch 8/10\n",
      "47/47 [==============================] - 0s 1ms/step - loss: 0.5801\n",
      "Epoch 9/10\n",
      "47/47 [==============================] - 0s 1ms/step - loss: 0.5801\n",
      "Epoch 10/10\n",
      "47/47 [==============================] - 0s 1ms/step - loss: 0.5801\n",
      "16/16 [==============================] - 0s 882us/step\n",
      "<class 'numpy.ndarray'>\n",
      "Epoch 1/10\n",
      "47/47 [==============================] - 1s 1ms/step - loss: 1.3872\n",
      "Epoch 2/10\n",
      "47/47 [==============================] - 0s 1ms/step - loss: 0.6993\n",
      "Epoch 3/10\n",
      "47/47 [==============================] - 0s 1ms/step - loss: 0.5979\n",
      "Epoch 4/10\n",
      "47/47 [==============================] - 0s 1ms/step - loss: 0.5827\n",
      "Epoch 5/10\n",
      "47/47 [==============================] - 0s 1ms/step - loss: 0.5805\n",
      "Epoch 6/10\n",
      "47/47 [==============================] - 0s 1ms/step - loss: 0.5802\n",
      "Epoch 7/10\n",
      "47/47 [==============================] - 0s 1ms/step - loss: 0.5801\n",
      "Epoch 8/10\n",
      "47/47 [==============================] - 0s 1ms/step - loss: 0.5801\n",
      "Epoch 9/10\n",
      "47/47 [==============================] - 0s 1ms/step - loss: 0.5801\n",
      "Epoch 10/10\n",
      "47/47 [==============================] - 0s 1ms/step - loss: 0.5801\n",
      "16/16 [==============================] - 0s 936us/step\n",
      "<class 'numpy.ndarray'>\n",
      "Epoch 1/10\n",
      "47/47 [==============================] - 1s 1ms/step - loss: 1.3903\n",
      "Epoch 2/10\n",
      "47/47 [==============================] - 0s 1ms/step - loss: 0.7035\n",
      "Epoch 3/10\n",
      "47/47 [==============================] - 0s 1ms/step - loss: 0.5989\n",
      "Epoch 4/10\n",
      "47/47 [==============================] - 0s 1ms/step - loss: 0.5827\n",
      "Epoch 5/10\n",
      "47/47 [==============================] - 0s 1ms/step - loss: 0.5804\n",
      "Epoch 6/10\n",
      "47/47 [==============================] - 0s 1ms/step - loss: 0.5802\n",
      "Epoch 7/10\n",
      "47/47 [==============================] - 0s 1ms/step - loss: 0.5801\n",
      "Epoch 8/10\n",
      "47/47 [==============================] - 0s 1ms/step - loss: 0.5801\n",
      "Epoch 9/10\n",
      "47/47 [==============================] - 0s 1ms/step - loss: 0.5800\n",
      "Epoch 10/10\n",
      "47/47 [==============================] - 0s 1ms/step - loss: 0.5802\n",
      "16/16 [==============================] - 0s 885us/step\n",
      "<class 'numpy.ndarray'>\n",
      "Epoch 1/10\n",
      "47/47 [==============================] - 1s 1ms/step - loss: 1.3879\n",
      "Epoch 2/10\n",
      "47/47 [==============================] - 0s 1ms/step - loss: 0.7017\n",
      "Epoch 3/10\n",
      "47/47 [==============================] - 0s 1ms/step - loss: 0.5990\n",
      "Epoch 4/10\n",
      "47/47 [==============================] - 0s 1ms/step - loss: 0.5831\n",
      "Epoch 5/10\n",
      "47/47 [==============================] - 0s 1ms/step - loss: 0.5805\n",
      "Epoch 6/10\n",
      "47/47 [==============================] - 0s 1ms/step - loss: 0.5802\n",
      "Epoch 7/10\n",
      "47/47 [==============================] - 0s 1ms/step - loss: 0.5801\n",
      "Epoch 8/10\n",
      "47/47 [==============================] - 0s 1ms/step - loss: 0.5801\n",
      "Epoch 9/10\n",
      "47/47 [==============================] - 0s 1ms/step - loss: 0.5801\n",
      "Epoch 10/10\n",
      "47/47 [==============================] - 0s 1ms/step - loss: 0.5801\n",
      "16/16 [==============================] - 0s 1ms/step\n",
      "<class 'numpy.ndarray'>\n",
      "Epoch 1/10\n",
      "47/47 [==============================] - 1s 2ms/step - loss: 1.3881\n",
      "Epoch 2/10\n",
      "47/47 [==============================] - 0s 2ms/step - loss: 0.7014\n",
      "Epoch 3/10\n",
      "47/47 [==============================] - 0s 1ms/step - loss: 0.5979\n",
      "Epoch 4/10\n",
      "47/47 [==============================] - 0s 2ms/step - loss: 0.5831\n",
      "Epoch 5/10\n",
      "47/47 [==============================] - 0s 3ms/step - loss: 0.5806\n",
      "Epoch 6/10\n",
      "47/47 [==============================] - 0s 1ms/step - loss: 0.5801\n",
      "Epoch 7/10\n",
      "47/47 [==============================] - 0s 1ms/step - loss: 0.5801\n",
      "Epoch 8/10\n",
      "47/47 [==============================] - 0s 1ms/step - loss: 0.5800\n",
      "Epoch 9/10\n",
      "47/47 [==============================] - 0s 1ms/step - loss: 0.5801\n",
      "Epoch 10/10\n",
      "47/47 [==============================] - 0s 1ms/step - loss: 0.5800\n",
      "16/16 [==============================] - 0s 871us/step\n",
      "<class 'numpy.ndarray'>\n",
      "Epoch 1/10\n",
      "47/47 [==============================] - 1s 1ms/step - loss: 1.3892\n",
      "Epoch 2/10\n",
      "47/47 [==============================] - 0s 1ms/step - loss: 0.7024\n",
      "Epoch 3/10\n",
      "47/47 [==============================] - 0s 1ms/step - loss: 0.5980\n",
      "Epoch 4/10\n",
      "47/47 [==============================] - 0s 1ms/step - loss: 0.5826\n",
      "Epoch 5/10\n",
      "47/47 [==============================] - 0s 2ms/step - loss: 0.5805\n",
      "Epoch 6/10\n",
      "47/47 [==============================] - 0s 1ms/step - loss: 0.5802\n",
      "Epoch 7/10\n",
      "47/47 [==============================] - 0s 1ms/step - loss: 0.5801\n",
      "Epoch 8/10\n",
      "47/47 [==============================] - 0s 1ms/step - loss: 0.5801\n",
      "Epoch 9/10\n",
      "47/47 [==============================] - 0s 2ms/step - loss: 0.5801\n",
      "Epoch 10/10\n",
      "47/47 [==============================] - 0s 1ms/step - loss: 0.5801\n",
      "16/16 [==============================] - 0s 968us/step\n",
      "<class 'numpy.ndarray'>\n",
      "Epoch 1/10\n",
      "47/47 [==============================] - 1s 1ms/step - loss: 1.3897\n",
      "Epoch 2/10\n",
      "47/47 [==============================] - 0s 1ms/step - loss: 0.7018\n",
      "Epoch 3/10\n",
      "47/47 [==============================] - 0s 1ms/step - loss: 0.5989\n",
      "Epoch 4/10\n",
      "47/47 [==============================] - 0s 1ms/step - loss: 0.5830\n",
      "Epoch 5/10\n",
      "47/47 [==============================] - 0s 1ms/step - loss: 0.5804\n",
      "Epoch 6/10\n",
      "47/47 [==============================] - 0s 1ms/step - loss: 0.5801\n",
      "Epoch 7/10\n",
      "47/47 [==============================] - 0s 1ms/step - loss: 0.5802\n",
      "Epoch 8/10\n",
      "47/47 [==============================] - 0s 1ms/step - loss: 0.5801\n",
      "Epoch 9/10\n",
      "47/47 [==============================] - 0s 1ms/step - loss: 0.5801\n",
      "Epoch 10/10\n",
      "47/47 [==============================] - 0s 1ms/step - loss: 0.5802\n",
      "16/16 [==============================] - 0s 907us/step\n",
      "<class 'numpy.ndarray'>\n",
      "Epoch 1/10\n",
      "47/47 [==============================] - 1s 1ms/step - loss: 1.3846\n",
      "Epoch 2/10\n",
      "47/47 [==============================] - 0s 1ms/step - loss: 0.6996\n",
      "Epoch 3/10\n",
      "47/47 [==============================] - 0s 1ms/step - loss: 0.5973\n",
      "Epoch 4/10\n",
      "47/47 [==============================] - 0s 1ms/step - loss: 0.5827\n",
      "Epoch 5/10\n",
      "47/47 [==============================] - 0s 1ms/step - loss: 0.5806\n",
      "Epoch 6/10\n",
      "47/47 [==============================] - 0s 1ms/step - loss: 0.5802\n",
      "Epoch 7/10\n",
      "47/47 [==============================] - 0s 1ms/step - loss: 0.5801\n",
      "Epoch 8/10\n",
      "47/47 [==============================] - 0s 1ms/step - loss: 0.5801\n",
      "Epoch 9/10\n",
      "47/47 [==============================] - 0s 1ms/step - loss: 0.5801\n",
      "Epoch 10/10\n",
      "47/47 [==============================] - 0s 1ms/step - loss: 0.5801\n",
      "16/16 [==============================] - 0s 900us/step\n",
      "<class 'numpy.ndarray'>\n",
      "Epoch 1/10\n",
      "47/47 [==============================] - 1s 2ms/step - loss: 1.3898\n",
      "Epoch 2/10\n",
      "47/47 [==============================] - 0s 1ms/step - loss: 0.7015\n",
      "Epoch 3/10\n",
      "47/47 [==============================] - 0s 1ms/step - loss: 0.5982\n",
      "Epoch 4/10\n",
      "47/47 [==============================] - 0s 1ms/step - loss: 0.5828\n",
      "Epoch 5/10\n",
      "47/47 [==============================] - 0s 1ms/step - loss: 0.5806\n",
      "Epoch 6/10\n",
      "47/47 [==============================] - 0s 1ms/step - loss: 0.5801\n",
      "Epoch 7/10\n",
      "47/47 [==============================] - 0s 1ms/step - loss: 0.5801\n",
      "Epoch 8/10\n",
      "47/47 [==============================] - 0s 1ms/step - loss: 0.5801\n",
      "Epoch 9/10\n",
      "47/47 [==============================] - 0s 1ms/step - loss: 0.5800\n",
      "Epoch 10/10\n",
      "47/47 [==============================] - 0s 1ms/step - loss: 0.5801\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "<class 'numpy.ndarray'>\n",
      "0.0\n"
     ]
    }
   ],
   "source": [
    "import numpy as np \n",
    "import pandas as pd\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "import tensorflow as tf\n",
    "\"\"\"Suppose f (x) = |x| is a target function we want to approximate using a ReLU network. \n",
    "Since |x|=ReLU(x)+ReLU(−x), a 2-layer ReLU network of width 2 can exactly represent |x|. \n",
    "However, when we train a deep ReLU network, we frequently observe\"\"\"\n",
    "np.random.seed(1999)\n",
    "num_simulations= 1000\n",
    "mini_batch_size = 64\n",
    "\n",
    "def function_fx(x):\n",
    "    return abs(x)\n",
    "\n",
    "# should we draw the 3k trainig points or use the same training points of for all of the simulations.\n",
    "training_set = []\n",
    "for i in range(3000):\n",
    "    x_y = []\n",
    "    x_y.append(np.random.uniform(-np.sqrt(7),np.sqrt(7)))\n",
    "    x_y.append(function_fx(x_y[0]))\n",
    "    training_set.append(x_y)\n",
    "\n",
    "train_x = np.array(training_set)[:,0].reshape(-1,1)\n",
    "train_y = np.array(training_set)[:,1].reshape(-1,1)\n",
    "\n",
    "test_set = []\n",
    "for i in range(500):\n",
    "    x_y = []\n",
    "    x_y.append(np.random.uniform(-np.sqrt(7),np.sqrt(7)))\n",
    "    x_y.append(function_fx(x_y[0]))\n",
    "    test_set.append(x_y)\n",
    "    \n",
    "test_x = np.array(test_set)[:,0].reshape(-1,1)\n",
    "test_y = np.array(test_set)[:,1].reshape(-1,1)\n",
    "    \n",
    "increment = 0\n",
    "for sim in range(10): # doing 10 times to test it \n",
    "    model = tf.keras.Sequential()\n",
    "    model.add(tf.keras.layers.Dense(units = 2,activation = 'relu'))\n",
    "    model.add(tf.keras.layers.Dense(units = 2,activation = 'relu'))\n",
    "    model.add(tf.keras.layers.Dense(units = 2,activation = 'relu'))\n",
    "    model.add(tf.keras.layers.Dense(units = 2,activation = 'relu'))\n",
    "    model.add(tf.keras.layers.Dense(units = 2,activation = 'relu'))\n",
    "    model.add(tf.keras.layers.Dense(units = 2,activation = 'relu'))\n",
    "    model.add(tf.keras.layers.Dense(units = 2,activation = 'relu'))\n",
    "    model.add(tf.keras.layers.Dense(units = 2,activation = 'relu'))\n",
    "    model.add(tf.keras.layers.Dense(units = 2,activation = 'relu'))\n",
    "    model.add(tf.keras.layers.Dense(units = 2,activation = 'relu'))\n",
    "    model.add(tf.keras.layers.Dense(units = 1))\n",
    "    model.compile(optimizer='sgd', loss='mse')\n",
    "    # This builds the model for the first time:\n",
    "    model.fit(train_x, train_y, batch_size=64, epochs=10)\n",
    "    y_predict = model.predict(test_x)\n",
    "    print(type(y_predict))\n",
    "    result = np.all(y_predict == y_predict[0])\n",
    "    if result:\n",
    "        increment+=1\n",
    "    else: \n",
    "        continue\n",
    "\n",
    "    \n",
    "print(increment/10)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3.\n",
    " Instead of ReLU consider Leaky ReLU activation as defined below: 􏰀 z ifz>0\n",
    " References:\n",
    "φ(z)= 0.01z ifz≤0.\n",
    "Run the 1000 training simulations in part 2 with Leaky ReLU activation and keeping everything else same. Again calculate the fraction of simulations that resulted in neural network collapse. Did Leaky ReLU help in preventing dying neurons ?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.5 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "2a0f08725565d2d00e70b9fae99b105541271f7be4baa13607a6e77e1d2c8c73"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
