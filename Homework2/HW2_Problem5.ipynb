{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem 5 - Weight Initialization, Dead Neurons, Leaky ReLU\n",
    "\n",
    "Read the two blogs, one by Andre Pernunicic and other by Daniel Godoy on weight initialization. You will reuse the code at github repo linked in the blog for explaining vanishing and exploding gradients. You can use the same 5 layer neural network model as in the repo and the same dataset.\n",
    "\n",
    "• Andre Perunicic. Understand neural network weight initialization. Available at https://intoli.com/blog/neural-network-initialization/\n",
    "• Daniel Godoy. Hyper-parameters in Action Part II — Weight Initializers.\n",
    "• Initializers - Keras documentation. https://keras.io/initializers/.\n",
    "• Lu Lu et al. Dying ReLU and Initialization: Theory and Numerical Examples ."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Explain vanishing gradients phenomenon using standard normalization with different values of standard deviation as given in the reference. Train the model with tanh and sigmoid activation functions. Next, show how Xavier (aka Glorot normal) initialization of weights helps in dealing with this problem. You should plot the gradients at each of the 5 layers for all 4 experiments to answer this question."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. The dying ReLU is a kind of vanishing gradient, which refers to a problem when ReLU neurons become\n",
    "inactive and only output 0 for any input. In the worst case of dying ReLU, ReLU neurons at a certain\n",
    "layer are all dead, i.e., the entire network dies and is referred as the dying ReLU neural networks in\n",
    "Lu et al (reference below). A dying ReLU neural network collapses to a constant function. Show this\n",
    "phenomenon using any one of the three 1-dimensional functions in page 13 of Lu et al. Use a ReLU\n",
    "network with 10 hidden layers, each of width 2 (hidden units per layer). Use minibatch of 64 and draw\n",
    "training data uniformly from [sqrt(− 7),sqrt() 7]. Perform 1000 independent training simulations each with\n",
    "3,000 training points. Out of these 1000 simulations, what fraction resulted in neural network collapse. Is your answer close to over 90% as was reported in Lu et al.? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Instead of ReLU consider Leaky ReLU activation as defined below: 􏰀 z ifz>0\n",
    " References:\n",
    "φ(z)= 0.01z ifz≤0.\n",
    "Run the 1000 training simulations in part 2 with Leaky ReLU activation and keeping everything else same. Again calculate the fraction of simulations that resulted in neural network collapse. Did Leaky ReLU help in preventing dying neurons ?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.7 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.7.7"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
