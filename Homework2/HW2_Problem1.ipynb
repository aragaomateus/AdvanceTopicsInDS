{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem 1 - Perceptron\n",
    "Consider a 2-dimensional data set in which all points with x1 > x2 belong to the positive class, and all points with x1 ≤ x2 belong to the negative class. Therefore, the true separator of the two classes is linear hyperplane (line) defined by x1 − x2 = 0. Now create a training data set with 10 points randomly generated inside the unit square in the positive quadrant. Label each point depending on whether or not the first coordinate x1 is greater than its second coordinate x2. Now consider the following loss function for training pair (X ̄,y) and weight vector W ̄ :\n",
    "L=max{0,a−y(W ̄ ·X ̄)},\n",
    "\n",
    "where the test instances are predicted as yˆ = {W ̄ · X ̄}. For this problem, W ̄ = [w1,w2], X ̄ = [x1,x2] and yˆ = (w1x1 +w2x2). A value of a = 0 corresponds to the perceptron criterion and a value of a = 1 corresponds to hinge-loss.\n",
    "\n",
    "• Perceptron Algorithm in Python\n",
    "Available at https://medium.com/hackernoon/implementing-the-perceptron-algorithm-from-scratch- in-python-48be2d07b1c0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. You need to implement the perceptron algorithm without regularization, train it on the 10 points above, and test its accuracy on 5000 randomly generated points inside the unit square. Generate the test points using the same procedure as the training points. You need to have your own implementation of the perceptron algorithm, using the perceptron criterion loss function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generating the training set:\n",
    "# 10 points inside the positive quadrant of the unit square.\n",
    "import numpy as np\n",
    "\n",
    "np.random.seed(1999)\n",
    "\n",
    "def set_generator(size, quadrant):\n",
    "    import numpy as np\n",
    "    training_set = []\n",
    "    # sorry I over complicated the code in this step\n",
    "    if quadrant == 1:\n",
    "        for sample in range(size):\n",
    "            training_set.append([np.random.uniform(0,1),np.random.uniform(0,1)])\n",
    "    elif quadrant == 2:\n",
    "        for sample in range(size):\n",
    "            training_set.append([np.random.uniform(-1,0),np.random.uniform(0,1)])\n",
    "    elif quadrant == 3:\n",
    "        for sample in range(size):\n",
    "            training_set.append([np.random.uniform(-1,0),np.random.uniform(-1,0)])\n",
    "    elif quadrant == 4:\n",
    "        for sample in range(size):\n",
    "            training_set.append([np.random.uniform(0,1),np.random.uniform(-1,0)])\n",
    "    elif quadrant == 'whole':\n",
    "        for sample in range(size):\n",
    "            training_set.append([np.random.uniform(-1,1),np.random.uniform(-1,1)])\n",
    "        \n",
    "    for x in training_set:\n",
    "        if x[0] > x[1]:\n",
    "            x.append(1)\n",
    "        else:\n",
    "            x.append(-1)\n",
    "    return np.array(training_set)\n",
    "\n",
    "training = set_generator(10,quadrant = 1)\n",
    "training_x = training[:,:2]\n",
    "training_y = training[:,2:]\n",
    "# Generating the test set:\n",
    "# 5k points inside the unit square.\n",
    "\n",
    "test = set_generator(5000,quadrant = 'whole')\n",
    "test_x = test[:,:2]\n",
    "test_y = test[:,2:]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The perceptron\n",
    "from sklearn.metrics import accuracy_score\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "class perceptron:\n",
    "    \n",
    "    import numpy as np\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.weights = None \n",
    "        self.a = None\n",
    "        \n",
    "    # with this function I am declaring the loss function used for learning.\n",
    "    def loss_function(self,X):\n",
    "        return np.sign(self.a - np.dot(self.weights,X))\n",
    "    \n",
    "    # X is the x values for the training set\n",
    "    # Y are the lables for the training set\n",
    "    # a is the factor that indicates is it is the perceptron criterion or hinge loss\n",
    "    # epoch is how many times we are going through training our weightss\n",
    "    np.random.seed(1000)\n",
    "    \n",
    "    def fit(self,X,Y,a,epoch):\n",
    "        \n",
    "        self.a = a\n",
    "        #lets randomly initialize the weights with 2 numbers from a \n",
    "        self.weights = [ np.random.uniform(-1,1) for i in range(len(X[0]))]\n",
    "        # in each epoch we go through the training set once.\n",
    "        for step in range(epoch):\n",
    "            for x,y in zip(X,Y):\n",
    "                prediction = self.loss_function(x)\n",
    "                error = int(y[0]) - prediction\n",
    "                if y[0] != prediction:\n",
    "                    self.weights = self.weights + (-0.1 * error * x )\n",
    "    \n",
    "    def predict(self,X):\n",
    "        # here we will have to round the number for the positive values\n",
    "        y_predicted = []\n",
    "        for x in X: \n",
    "            y_predicted.append(self.loss_function(x))\n",
    "        return np.array(y_predicted)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.964\n",
      "[ 1. -1. -1. ... -1.  1.  1.] [[ 1.]\n",
      " [-1.]\n",
      " [-1.]\n",
      " ...\n",
      " [-1.]\n",
      " [-1.]\n",
      " [ 1.]]\n"
     ]
    }
   ],
   "source": [
    "neuron = perceptron()\n",
    "\n",
    "neuron.fit(training_x,training_y,0,10000)\n",
    "\n",
    "y_predicted = neuron.predict(test_x)        \n",
    "print(accuracy_score(y_predicted, np.array(test_y)))\n",
    "\n",
    "# output = zip(y_predicted,test_y)\n",
    "# for i,j in output:\n",
    "#     print(i,j) \n",
    "print(y_predicted, np.array(test_y))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Change the loss function from perceptron criterion to hinge-loss in your implementation for training, and repeat the accuracy computation on the same test points above. Regularization is not used\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6372\n"
     ]
    }
   ],
   "source": [
    "neuron2= perceptron()\n",
    "\n",
    "neuron2.fit(training_x,training_y,1,10000)\n",
    "\n",
    "y_predicted2 = neuron2.predict(test_x)        \n",
    "print(accuracy_score(y_predicted2, np.array(test_y)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8006\n",
      "0.7574\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/aragaom/opt/anaconda3/lib/python3.8/site-packages/sklearn/utils/validation.py:72: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  return f(**kwargs)\n",
      "/Users/aragaom/opt/anaconda3/lib/python3.8/site-packages/sklearn/utils/validation.py:72: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  return f(**kwargs)\n"
     ]
    }
   ],
   "source": [
    "# this code was just for the sake of double checking if my results for \n",
    "\n",
    "# import numpy as np\n",
    "# from sklearn.linear_model import SGDClassifier\n",
    "# from sklearn.preprocessing import StandardScaler\n",
    "# from sklearn.pipeline import make_pipeline\n",
    "\n",
    "# clf = SGDClassifier(max_iter=1000, tol=1e-3, loss='perceptron')\n",
    "# clf.fit(training_x, training_y)\n",
    "# predicted=  clf.predict(test_x)\n",
    "\n",
    "# print(accuracy_score(predicted, np.array(test_y)))\n",
    "\n",
    "# clf = SGDClassifier(max_iter=1000, tol=1e-3, loss='hinge')\n",
    "# clf.fit(training_x, training_y)\n",
    "# predicted2=  clf.predict(test_x)\n",
    "\n",
    "# print(accuracy_score(predicted2, np.array(test_y)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. In which case do you obtain better test accuracy and why?\n",
    "### Answer: \n",
    "The perceptron criterion gave me the better accuracy. Because..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. In which case do you think that the classification of the same 5000 test instances will not change significantly by using a different set of 10 training points?\n",
    "### Answer:\n",
    "TO DO ..."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.5 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "2a0f08725565d2d00e70b9fae99b105541271f7be4baa13607a6e77e1d2c8c73"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
